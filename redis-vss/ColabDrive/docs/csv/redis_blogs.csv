id,url,title,date,author,text
1,https://redis.com/blog/velocity-based-data-architectures/,An Introduction to Velocity-Based Data Architectures,"August 7, 2023",Amine El Kouhen,"There are several ways to classify and understand data architectures, each with its own pros and cons. They can help you make an informed decision about the best design for your needs. Here, I explain velocity-based data architectures, and where they fit in the grand scheme of things.The two most popular velocity-based architectures are Lambda and Kappa. Data architectures are also classified based on their operational mode or topology, including data fabric, data hub, and data mesh–but I leave that explanation to a later blog post.Data architecture is an element in enterprise architecture, inheriting its main properties: processes, strategy, change management, and evaluating trade-offs. According to the Open Group Architecture Framework, data architecture is “a description of the structure and interaction of the enterprise’s major types and sources of data, logical data assets, physical data assets, and data management resources.”Per the Data Management Body of Knowledge, data architecture is the process of “identifying the data needs of the enterprise (regardless of structure) and designing and maintaining the master blueprints to meet those needs.” It uses master blueprints to guide data integration, control data assets, and align data investments with business strategy.Not every process is a good one. Bad data architecture is tightly coupled, rigid, and overly centralized. It uses the wrong tools for the job, which hampers development and change management.Data velocity refers to how quickly data is generated, how quickly that data moves, and how soon it can be processed into usable insights.Depending on the velocity of data they process, data architectures often are classified into two categories: Lambda and Kappa.Lambda data architectures were developed in 2011 by Nathan Marz, the creator of Apache Storm, to solve the challenges of large-scale real-time data processing.The term Lambda, derived from lambda calculus (λ), describes a function that runs in distributed computing on multiple nodes in parallel. Lambda data architecture provides a scalable, fault-tolerant, and flexible system for processing large amounts of data. It allows access to batch-processing and stream-processing methods in a hybrid way.The Lambda architecture is ideal when you have a variety of workloads and velocities. Because it can handle large volumes of data and provide low-latency query results, it is suitable for real-time analytics applications such as dashboards and reporting. The architecture is useful for batch processing (cleansing, transforming, data aggregation), for stream processing tasks ( event handling, developing machine learning models, anomaly detection,  fraud prevention), and for building centralized repositories (known as “data lakes”) to store structured and unstructured information.Lambda architecture’s critical distinction is that it uses two separate processing systems to handle different types of data processing workloads. The first is a batch processing system that stores the results in a centralized data store (such as a data warehouse or a data lake). The second system in a Lambda architecture is a stream processing system, which processes data in real-time as it arrives and stores the results in a distributed data store.Lambda architecture solves the problem of computing arbitrary functions, whereby the system has to evaluate the data processing function for any given input (whether in slow motion or in real-time). Furthermore, it provides fault tolerance by ensuring that the results from either system can be used as input into the other if one fails or becomes unavailable. The efficiency of this architecture becomes evident in high throughput, low latency, and near-real-time applications.Lambda architecture consists of an ingestion layer, a batch layer, a speed layer (or stream layer), and a serving layer.Lambda architectures offer many advantages, such as scalability, fault tolerance, and the flexibility to handle a wide range of data processing workloads (batches and streams). But it also has drawbacks:In 2014, when he was still working at LinkedIn, Jay Kreps pointed out some Lambda architecture drawbacks. The discussion led the Big Data community to an alternative that uses fewer code resources.The principal idea behind Kappa (named after the Greek letter ϰ, used in mathematics to represent a loop or cycle) is that a single technology stack can be used for both real-time and batch data processing. The name reflects the architecture’s emphasis on continuous data processing or reprocessing in contrast to a batch-based approach.At its core, Kappa relies on streaming architecture. Incoming data is first stored in an event streaming log. It then is processed continuously by a stream processing engine ( Kafka, for instance) either in real-time or ingested into another analytics database or business application. Doing so uses various communication paradigms such as real-time, near real-time, batch, micro-batch, and request response.Kappa architecture is designed to provide a scalable, fault-tolerant, and flexible system for processing large amounts of data in real-time. The Kappa architecture is considered a simpler alternative to the Lambda architecture; it uses a single technology stack to handle both real-time and historical workloads, and it treats everything as streams. The primary motivation for the Kappa architecture was to avoid maintaining two separate code bases (pipelines) for the batch and speed layers. This allows it to provide a more streamlined and simplified data processing pipeline while still providing fast and reliable access to query results.Data reprocessing is a key requirement of Kappa, making visible the effects of any changes in the source side on the outcomes. Consequently, the Kappa architecture is composed of only two layers: the stream layer and the serving one.In Kappa architecture, there is only one processing layer: the stream processing layer. This layer is responsible for collecting, processing, and storing live-streaming data. This approach eliminates the need for batch-processing systems. Instead, it uses an advanced stream processing engine (such as Apache Flink, Apache Storm, Apache Kafka, or Apache Kinesis) to handle high volumes of data streams and to provide fast, reliable access to query results.The stream processing layer has two components:For almost every use case, real-time data beats slow data. Nevertheless, Kappa architecture should not be taken as a substitute for Lambda architecture. On the contrary, you should consider Kappa architecture in circumstances where the batch layer’s active performance is not necessary to meet the standard quality of service.Kappa architectures promise scalability, fault tolerance, and streamlined management. However, it also has disadvantages. For instance, Kappa architecture is theoretically simpler than Lambda but it can still be technically complex for businesses unfamiliar with stream processing frameworks.The major drawback of Kappa, from my point of view, is the cost of infrastructure while scaling the event streaming platform. Storing high volumes of data in an event streaming platform can be costly and raise other scalability issues, especially when the data volume is measured in terabytes or petabytes.Moreover, the lag between event time and processing time inevitably generates late-arriving data as a side effect. Kappa architecture will need, thus, a set of mechanisms, such as watermarking, state management, reprocessing, or backfilling, to overcome this issue.Lambda and Kappa were attempts to overcome the shortcomings of the Hadoop ecosystem in the 2010s by trying to integrate complex tools that were not inherently compatible. Both approaches struggle to resolve the fundamental challenge of reconciling batch and streaming data. Yet, Lambda and Kappa have provided inspiration and a foundation for further advancements.Unifying multiple code paths is a significant challenge in managing batch and stream processing. Even with the Kappa architecture’s unified queuing and storage layer, developers need to use different tools to collect real-time statistics and run batch aggregation jobs. Today, they are working to address this challenge. For instance, Google has made significant progress by developing the dataflow model and its implementation, the Apache Beam framework.The fundamental premise of the dataflow model is to treat all data as events and perform aggregations over different types of windows. Real-time event streams are unbounded data, while data batches are bounded event streams that have natural windows.Data engineers can choose from different windows, such as sliding or tumbling, for real-time aggregation. The dataflow model enables real-time and batch processing to occur within the same system, using almost identical code.The idea of “batch as a special case of streaming” has become increasingly widespread, with frameworks like Flink and Spark adopting similar approaches.There’s another twist on the data architecture discussion in regard to velocity models: the suitable design choices for working with the Internet of Things (IoT). But I’ll leave that for a separate discussion.It’s clear that the debate over how best to structure our approach to handling data is far from over. We’re just getting started! Consult our white paper, Best Practices for a Modern Data Layer in Financial Services, on the best ways to modernize a rigid and slow IT legacy system and turn it into a modern data architecture."
2,https://redis.com/blog/choose-microservice-monitoring-tool/,How to Choose a Microservices Monitoring Tool,"August 8, 2023",Esther Schindler,"Microservices allow developers to break down their applications into smaller, loosely coupled services that are developed, deployed, and scaled independently. But you need a monitoring tool to track whether the software works correctly–and that means you need useful criteria for choosing such a tool.You’re used to tracking application performance to confirm that it functions correctly, but microservices adds a new twist. Monitoring is a critical aspect of managing any microservices architecture. By definition, there are a lot of independent parts.But how do you choose the best microservice monitoring tool for your business? We’re not here to play favorites or to endorse any tool in particular. What we do have, however, is a whole lot of experience in this field, which we are happy to share.Before you choose a tool, contemplate your motivations for acquiring one. Your team should discuss, “What problems are we trying to solve or prevent?” That leads to, “What data do we need to examine to determine if we are solving or preventing those problems?” The answers help you identify what to monitor–and what you can ignore (or pay less attention to).Be intentional about what you monitor. Have a reason. Don’t adopt the attitude, “Monitor all the things just in case it might be useful.” Most teams have limited resources, which means that it isn’t possible to monitor all the things anyway; at best, you end up with alert fatigue.That said: Your expectations may not match reality. It’s hard to tell ahead of time what’s going to be useful when an unexpected thing breaks. It is not clear what needs to be monitored until everything is on fire and you try to figure out what’s going on. You need a mix of “think carefully” and “adjust given experience.”Any type of application monitoring tool has a host of features. You may not need all of them. It’s a good idea to start with the top criteria, as identified by our Redis experts and by experienced practitioners (the people who have the scars).It should scale. As your microservices architecture grows, so do your monitoring needs. The last thing you want is a tool that can’t keep up with the load. Make sure your monitoring system can go down without bringing down your microservice!It needs to collect the right data and analyze it. Look carefully at the data the tool collects and how it presents that information.A robust monitoring tool collects and analyzes data from every nook and cranny of a distributed system–but it shouldn’t overwhelm you with noisy, irrelevant information. It should provide you with comprehensive insights that deserve to be called “insights,” including performance metrics, logs, and traces.For microservices architectures, prioritize distributed tracing. Debugging issues that span multiple microservices can be a nightmare. Distributed tracing helps you track the flow of requests across services, which assists in identifying performance bottlenecks and understanding complex interactions. For example, ensure every log message/record/line has an attributable traceid attached to it, and use a system that lets you aggregate views.It should integrate with other tools you use without fussy setups or custom code. Perhaps more than any other application, a monitoring tool should play well with others.Similarly, look at the process of migration to the new monitoring tool from your existing provider, including data structuring requirements. Research what it would take to switch to another tool if this one doesn’t work out.  Learn what the API is like, because you’re bound to need it at some point. Consider future standards support, such as OpenTelemetry.Please, let it be easy to learn and easy to use (which are not the same things). Who wants to struggle to learn yet another tool? Navigating through distributed systems is complicated enough; your monitoring tool should simplify things, not add to the system’s complexity. Configuration should not be a pain. Peer closely at its dashboards and visualizations to decide if they are as intuitive as the vendor promises.It should set sensible alerts and notifications. When a storm is brewing, you need to know immediately! Your monitoring tool should offer robust alerting and notification features, so you can take action before minor issues turn into big problems.It has to fit your budget. While you want the best tool for your distributed system, you don’t want uncomfortable conversations with the CFO. That’s true for any IT expenditure, but especially so here, because the cost and pricing models vary widely. Unexpected usage has been known to create incidents of, shall we say, accidental overspending. Pay-per-user models sometimes create awkward decisions about who gets access.A microservice monitoring tool should offer visibility across the entire microservices ecosystem, including performance metrics, resource utilization, service mesh data, custom metrics, and error rates.The ideal tool should excel at collecting, storing, and analyzing data from distributed systems, providing actionable insights into the health and performance of each microservice. It should seamlessly integrate with other tools and systems, such as logging systems, alerting tools, and incident management platforms.Practically speaking, if you use Redis – for microservices or other uses – this is a good starter set for what to monitor. Your dashboard might include these items, configured to alert you when any metric spikes significantly.You have a lot of choices. Each of these microservice monitoring tools offers unique features. Perhaps these links can give you a head start on your shopping list – though there are many more options than we include here.Which is right for you? That’s your call.As with any other software choice, ultimately the only thing that matters is that it works for you.There’s no definitive right or wrong choice. The key question is, “Is this the right tool for my specific project?” The crucial factor is finding a tool that aligns with your project’s current and future needs and making an informed choice. Ideally, the tool you choose helps you maintain a healthy and efficient microservices environment, ultimately delivering reliable, high-performance applications. Redis works with all of them.And, we believe, you can take your microservices applications to the next level with Redis Enterprise. Read the Cache and Message Broker for Microservices solution brief to learn how to use caching with Redis Enterprise, explore top caching patterns, and use Redis Streams as a lightweight message broker for inter-services communication."
3,https://redis.com/blog/jedis-vs-lettuce-an-exploration/,Jedis vs. Lettuce: An Exploration,"September 12, 2022",Redis,"I’m an explorer by heart, so when I have to make a technical decision—like, say, choosing a Redis client—I go a-spelunking. Herein is the account of my exploration of the rhyming duo of Java clients: Jedis versus Lettuce.Let’s start with the basics, and examine each client, to understand the differences between the two.Lettuce is a Redis Java client that is fully non-blocking. It supports both synchronous and asynchronous communication. Its complex abstractions allow you to scale products easily.Consider Lettuce as a more advanced client that supports Cluster, Sentinel, Pipelining, and codecs.Jedis is a client library inside Redis that’s designed for performance and ease of use. Jedis is a lightweight offering compared to other Redis Java clients; it offers fewer features but can still handle large amounts of memory.Due to its simpler functionality, Jedis is easier to use, but it only works with clusters synchronously. If you choose Jedis, you may find it less challenging to focus on the application and data rather than the data storage mechanism.The gnomic goal of profit, like underpants, is always there. But the part that you can benefit from is the selection criteria. It will allow us to decide when Jedis is the right call and when Lettuce is the way to go. This is super important because we all know the answer to any question when selecting tools is, “it depends.”Jedis is a client library inside Redis that’s designed for performance and ease of use. Jedis is a lightweight offering compared to other Redis Java clients; it offers fewer features but can still handle large amounts of memory.Due to its simpler functionality, Jedis is easier to use, but it only works with clusters synchronously. If you choose Jedis, you may find it less challenging to focus on the application and data rather than the data storage mechanism.My comparison plan was simple:The gnomic goal of profit, like underpants, is always there. But the part that you can benefit from is the selection criteria. It allows us to decide when the Java client, Jedis, is the right call and when Lettuce software is the way to go. This is super important because we all know the answer to any tool selection question is, “It depends.”To get to the bottom of the Jedis versus Lettuce debate, let’s compare code for the simplest of all exercises: setting and getting a value from a single instance of Redis.First, we do this with Jedis:Looking at the code, this is pretty simple. Create a connection. Use it. Close it.Next, we do it with Lettuce:This looks a bit more involved. There’s a Java client, a connection, and a command object. And their names and templated nature suggest that there might be multiple varieties of them. Maybe in addition to a  StatefulRedisConnection<String, String> type, we have a stateless variety that takes a byte[]]? (Spoiler: there are multiple connection types for clustering and master/replica configurations, but not stateless ones.)Once you’re beyond the setup and teardown, however, it’s the same basic code in either client: Create a connection. Use it. Close it.Right now, for something as simple as this, Jedis looks easier. That makes sense since it has less code. But I am certain that the Lettuce software has all this stuff for a reason—probably to handle more advanced scenarios.Jedis can handle multi-threaded applications just fine, but a Jedis connection is not thread-safe. So don’t share them. If you share a Jedis connection across threads, Redis blurts out all sorts of protocol errors, like:expected '$' but got ' 'To solve these sorts of problems, use JedisPool—a thread-safe object that dispenses thread-unsafe Jedis objects. Using it is simple, like the rest of Jedis. Just ask for a thread and return it to the pool via .close() when you’re done. Here it is in action:Each of these Jedis objects encapsulates a single connection to Redis, so (depending on how large your pool is) there may be blocking or idle connections. Furthermore, these connections are synchronous, so there’s always a degree of idleness.I feel like I should talk about clustering, but there’s not much to say—at least in terms of comparisons. There’s plenty of capability to discuss, but both libraries support clustering. Unsurprisingly, Jedis is easier to use, but it works with clusters only synchronously. The Lettuce software is more difficult to use but is capable of synchronous, asynchronous, and reactive interaction with the cluster.This is the recurring theme. And that shouldn’t be surprising. By its own admission, “Jedis was conceived to be EASY to use.” And Lettuce states, “Lettuce is a scalable Redis client for building non-blocking Reactive applications” right on its homepage.Of course, if you’re using Redis Enterprise, you don’t have to worry about clustering as it’s handled server-side. Just use the non-clustered APIs of Jedis or Lettuce, manage your keys so they’re slotted to the correct shards, and you’re good to go."
4,https://redis.com/blog/introducing-triggers-and-functions/,Triggers and Functions: Bring Code Closer to Your Data,"August 15, 2023",Thomas Caudron and Pieter Cailliau,"Developers can use Redis to build and maintain real-time applications. You can create JavaScript functions that automatically execute code on data changes directly in the Redis database, and thus ensure a lower latency.In general, applications handle business logic operation, sending code for execution to the database. This is a slow process, because code flows from a client into the server each time a function is executed. The developer is responsible for maintaining code consistency across all applications that access the same database, whether the code is simple requests or complex data operations, and many times code is repeated across applications.Following the Redis manifesto guideline, We are against complexity, we had to take action and to find a solution to those challenges.Four years ago, we introduced RedisGears, our first programmability model within the platform.  Developers wrote and executed scripts where data lived. However, the scripts were ephemeral, provided by each client, and that could lead to inconsistencies.Following that direction, in Redis 7.0 we introduced the initial implementation of the scripting method with functions. Functions boosted usability and durability because they are part of the database, inheriting the level of replication and persistence of the data.And now, we are proud to present the next step in programmability. With Redis 7.2, we introduce triggers and functions. These enhance Redis’ programmability, expand server-side capabilities, improve how and when functions are executed in the database, and facilitate the execution of complex business logic directly where the data lives.Triggers and functions is a new generation of programmability available through Redis Stack. It allows developers to program, store, and automatically execute JavaScript code on data changes directly in a Redis database.This capability lets developers define events (called triggers) to execute functions closer to the data. That is, developers define business logic that executes in response to database events or commands. That speeds up the code and related interactions, because there is no  wait to bring code from clients into the database.It also speeds up reaction time to other events in Redis, such as keyspace notifications that are not handled in real time with other means such as Publish and Subscribe (Pub/Sub) events.Triggers and functions handle the distribution within a clustered database, installing the libraries on each shard and executing the functions based on where the key lives.We are also introducing remote functions. Remote functions allow you to perform read actions that can access data from any slot, even in a clustered database, so all your data is accessible from each function.Redis uses Lua for scripting and functions. There are many benefits to Lua, such as code reusability, but it is not a commonly used language among professional developers. According to the 2022 StackOverflow developer survey, only 3.2% of developers use Lua in a professional capacity.In contrast, two thirds of developers use JavaScript. Using a well-known language lowers the adoption barrier for new Redis developers. It’s one less thing to learn.Another benefit of triggers and functions is that it reduces the complexity of managing business logic across multiple applications.When multiple applications access the same database, developers have to coordinate how applications process data in a consistent manner. It is common to duplicate code in each application to validate data, to enrich search results, or to update the database when another application makes changes.With triggers and functions, there is no longer a need to duplicate code on multiple applications. Code always executes in the same way, on-demand or initiated by an event in the database.Until now, reacting to database events within Redis required developers to rely on the Pub/Sub mechanism. While Pub/Sub has a lot of advantages, it isn’t always the right choice. In particular, Pub/Sub is not real-time. A client has to actively listen to the events; if the client is not listening, the events are lost.Now developers can register keyspace triggers that execute based on a key prefix and event type. The trigger can be executed in an atomic way so that no other Redis event is processed between the event and the business logic.It’s always easier to understand things with a practical example. Here, we introduce registering a function and a trigger. A function is executed when it is called via the TFCALL command; a trigger is executed based on the events in Redis.The prologue defines that we use the js engine, the library name is lib, and the minimum required Triggers and Functions API is version 1.0.Next, we create a function that returns the result of a Redis command. The client gives access to execute Redis commands within our function. The data event contains the keys and arguments that can be provided when running the function.The Redis global variable allows you to register triggers and functions, and to log to the log file. We register the function with a name with which we call it when it executes.The full JavaScript file looks like this.Save this as lib.js.Then we register our function in triggers and functions using the TFUNCTION LOAD command. The TFUNCTION LOAD command also distributes the library in a clustered database.Now we can execute the function using the TFCALL command. The command gets the library name and the function name separated by a period.By doing so, you successfully created, registered, and triggered a function in the Redis database.We can extend this example with a keyspace trigger. We add a new registration that reacts to keys that are prefixed with 'fellowship:'. Add this code at the end of the lib.js file.Use the TFUNCTION LOAD command with the REPLACE argument to update an existing library. The TFUNCTION LOAD REPLACE command immediately updates all the clients using the Redis database, and they start using the new business logic.To test the new keyspace trigger, create a new key starting with fellowship: and check the fields using RedisInsight. The keyspace trigger is executed with the command, so the last_updated field is already added when the key is created.Join the public preview of triggers and functions with Redis Stack 7.2. Get started with Redis Stack in the cloud by creating a database on Redis Enterprise Cloud in the fixed tier within the Google Cloud/Asia Pacific (Tokyo) or AWS/Asia Pacific (Singapore) region, or deploy a self-managed instance from our download center.The General Availability for triggers and functions is planned for Redis 8. This will include feedback received from the preview users, as well as additional features, such as timed triggers and more debugging options.Comments and feedback are welcome on the Redis mailing list."
5,https://redis.com/blog/implementing-designing-microservices/,The Principles of Designing Microservices,"May 1, 2023",Redis,"So, you’ve evaluated your application’s state of affairs and have concluded that the adoption of microservices will improve overall performance and scalability. Great. What comes next? In this article, we outline the baseline considerations for microservices design and implementation.Microservices is a software architecture strategy that breaks down applications into a collection of decoupled, autonomous services. These independent application services communicate with one another through APIs. Each service is managed by its own team of domain experts so that every software development team can control its own development cycles, test and deploy on its own schedule, use its own enterprise tools and resources, and accelerate time to market.Our Microservice Architecture Key Concepts drill into the foundations of this concept and offers some practical advice for getting started, including evaluating the wisdom of microservice architecture adoption for your shop. Now it’s time to get into the weeds.The first step in designing a microservices architecture is surveying the lay of the land, so to speak. One of Developer.com’s top ten microservices design principles is the single responsibility principle, which dictates that each service needs to be responsible for and infuse all of its resources into the successful development of one function and one function only within the microservices based application.Software architects should consider conducting a domain analysis to map out how to compartmentalize each service and what elements need to be factored into the application stack. This domain analysis is known as domain-driven design (DDD). It applies patterns, such as the entity pattern and the aggregate pattern, to a single bounded context in order to identify a single domain’s boundaries with more calculated precision.As author and Agile Manifesto signatory Martin Fowler explains, DDD is “an approach to software development that centers the development on programming a domain model that has a rich understanding of the processes and rules of a domain.” In other words, you should build each microservice around a specific business function.Once you identify the domains and outline their boundaries, it’s time to define the variables that best suit the application stack.Creating a microservices tech stack is a bit ad hoc. You often have to use a host of tools, frameworks, and programming languages to implement it all into a cohesive yet loosely coupled system.Factor in these variables as you choose your tools:Narrowing down the best programming language to use for microservices comes down to your familiarity with the language, the libraries available for the features you need, and the suite of features each language provides. Obviously, it saves time and energy to choose languages that are already in your development team’s repertoire.According to a 2021 JetBrains survey on microservices, “the three most popular languages for microservices development are Java (41%), JavaScript (37%), and Python (25%).” Each of these popular programming languages has substantial developer online support, many examples of successful application development, running environments, such as Node.JS, and vast client libraries.Make sure the language is suited for the business problem at hand; for instance, Python is popular in data analytics, while JavaScript is a solid option for full-stack development.Understand more about intuitive object mapping and high-level client libraries in Introducing the Redis OM Client Libraries.When you choose a suitable database to use with the applications you build for a microservices architecture, keep scalability, availability, and security at the top of your mind. Choose a database that best supports the data model which you plan to use in your microservice. Your tech stack should scale to handle any application load, ensure availability with failover protocols, and secure the application from any malicious attacks.For more information on landing a high-performance database for microservice based applications, read Microservices and the Data Layer.Your business function may require your microservices to use synchronous interservice communication methods for certain operations and asynchronous communication for others. Several communication formats and protocols can be used to assist microservices communication, including HTTP/REST, gRPC, and AMQP.For asynchronous communications, an event-driven message broker leveraging Streams with consumer groups can help fortify scalability and reliability so applications can grow and no service is ever out of reach.For more on choosing the right communication tools and patterns, see What to Choose for Your Synchronous and Asynchronous Communication Needs.Each microservice team is responsible for monitoring application performance, which usually employs logging and observability tools to keep a pulse on operations. That lets developers and operations staff track the entire system, from application performance to message broker streams to database resource utilization.When using a message broker, consider using a logging stream where each microservice can publish messages. This way, you can connect your preferred logging and observability tools to the stream and monitor your application asynchronously without slowing things down.Learn how the right monitoring resources can combat system complexity in our 5 Microservices Misconceptions blog post.Want your microservice architecture to truly thrive? Here are five microservice application design principles to reference as you design an architecture built to ensure top performance.Loose coupling and strong cohesion can be explained both by the single responsibility principle mentioned earlier; giving each domain team one single responsibility helps fortify the cohesion within that one domain, making, ironically enough, a monolith of all the functions within that service to the point where they’re essentially inextricably linked. Each service has its own domain experts and tools, but can still communicate with each other via APIs and other protocols. It’s much like the way coworkers from different departments interact: You share information with one another when it helps get the work done, without being too chatty about irrelevant-to-others details.Business applications are rarely stagnant.  Software changes as new business needs arise, industry assumptions change, and technology capabilities offer more functionality. Microservices should be evolvable and adapt to new requirements when needed.Responding to change is one of the foundations of the Agile Manifesto and for good reason. The world changes. People change. So should your software.One reason to implement microservices is their ability to automate processes that improve overall scalability. With container orchestration systems like Kubernetes, you can deploy a microservice’s entire database from a single image alongside the microservice. With the assistance of a Kubernetes controller, these portability benefits can help DevOps teams to manage, schedule, and orchestrate an automatic container deployment.Implementing microservices requires that the services in any given application maintain their own decentralized data. Service boundaries should isolate all the logic and data pertaining to any single service from other services within an application.This is the same logic that permits containerized microservices to have independent deployments. According to Red Hat, this principle has its own set of naysayers who believe that the principle leads to a proliferation of data redundancy. But one of the greatest upsides to establishing these discrete boundaries is that “When a microservice carries its own data, any strange behavior is confined within the microservice.” Who needs the guesswork?Disruptions occur. Application services go down without warning. Fiber-optic-seeking backhoes take down network operations. People forget to renew domains. Systems are interrupted by data connection issues resulting from a firewall failure.Plan for all the ways that things can go pear-shaped. Do your best to account for potential failures at the implementation level. Design for resiliency, such as with the Circuit Breaker Pattern, to keep services from dropping when a microservice isn’t capable of performing a given operation.Infrastructure automation may be a substantial plus in favor of microservices, but operational disruptions are still a very real possibility. In Organizing the Chaos of Data, Redis’ Allen Terleto is joined by Mike Leone of Enterprise Strategy Group (ESG) and Jim Roomer of Google Cloud to pull back the curtain on database disasters. They reveal the path toward efficient data handling, featuring Redis on Google Cloud customer stories.Watch the webinar."
6,https://redis.com/blog/redis-enterprise-7-2-brings-exciting-innovations-that-operators-will-love/,Redis Enterprise 7.2 Brings Exciting Innovations That Operators Will Love,"August 17, 2023",Adi Shtatfeld,"Among the highlights for operators are better access control, troubleshooting features, and maintenance capabilities.The Redis 7.2 release has a lot of new features that matter to developers, and we go into many of the details elsewhere. But the new version has many new capabilities that make life easier for operators and system administrators–from better access control to troubleshooting in multi-tenant environments–and we expect these features to make you smile.To begin with, the new Redis Enterprise Cluster Manager makes your life easier.Imagine this scenario: You are on-call. You receive a phone call notifying you of a problem with an application or database. Your priority is to identify the source of the issue. That’s rarely fun.When browsing to the Cluster Manager user interface (UI), the default page shows the database list. You see all your databases. Databases whose status is different from “active” are listed at the top, followed by databases with alerts, sorted by the number of alerts.To help you further investigate the problem, the new Redis Enterprise Cluster Manager has an expanded view for each row in the database view. That gives you quick access to crucial information in a user-friendly manner.The main databases’ view when one of the databases is an expanded view looks like this:The expanded view provides key matrices, updated status and alerts, along with configuration. The expanded view lets you retain the context without disruptions. And you can compare metrics and data across multiple databases.The new Redis Enterprise Cluster Manager makes it easier to perform maintenance operations.The new release prominently displays shards consumption (out of the total entitled by the license) on the main cluster configuration screen. This helps you stay on top of your license usage. When your license nears expiration, the new UI makes it easier to replace it.We added an indication for the primary node, so each node is displayed along with its role in the cluster. That makes it easier to plan maintenance or cluster upgrades.New in version 7.2, the Redis Enterprise Cluster Manager helps you manage certificates by displaying essential information (such as expiration dates) and providing a convenient way to upload new certificates directly from the UI.When you create a database in the new Redis Enterprise Cluster Manager UI, you now can easily access and highlight the option to add additional capabilities, also known as modules.Redis Enterprise 7.2 adds more functionality. To simplify maintenance, the Cluster Manager UI specifies the minimum Redis database version required for each module. The modules management screen provides visibility to the databases that use a given functionality so you can highlight any version dependencies.Redis Enterprise 7.2 offers a host of exciting new features and improvements, including auto tiering, support for triggers and functions, expanded JavaScript and client support, and a lot of other enhancements. You can experience all the benefits by downloading the 7.2 release and starting a free 30-day trial today."
7,https://redis.com/blog/introducing-auto-tiering/,Auto Tiering Offers Twice the Throughput at Half the Latency for Large Datasets,"August 15, 2023",Alon Magrafta,"More and more applications rely on huge data collections – and those applications have to respond quickly. Redis Enterprise 7.2 makes it possible to create super-fast applications with no extra work on a developer’s part. What’s not to like?Organizations have always depended on their collected data, but those datasets are growing – particularly in analysis-heavy markets such as e-commerce, finance, location-based computing, and high-end gaming. In medical image analysis research, for example, the median dataset size grew by three to ten times between 2011 and 2018.One reason for Redis’s popularity is that it provides incredibly fast data access. It does so by storing data in memory, so applications can retrieve and manipulate data at the fastest speeds. The more data an application needs to process, the more memory it requires to store the dataset. Still, those applications have to respond at near-instant speeds, even when the data stores from which they draw are huge.When the amount of data an application accesses is measured in terabytes, developers have to cope with the limitations of in-memory processing. As a result, they turn to disk-based solutions to support Redis behind the scenes. Doing so forces the developers to build an entire data management system in their applications, which means they spend their time on extraneous tasks instead of their original goal of providing a performant application.There has to be a better option. And there is.Using Redis Enterprise’s auto tiering, developers can extend large volume databases beyond the limits of the existing DRAM in the cluster by using solid state disks (SSD) as part of available memory. Taking advantage of some clever programming on our part, Redis Enterprise identifies what data should be in-memory and what data should stay on SSDs at any given moment, doubling the throughput and cutting latencies in half what it did with previous solutions.Everything happens automatically. The developer doesn’t need to write extra code or learn another new technology. By combining dynamic RAM with fast external storage, Redis Enterprise makes it easy to use system resources efficiently while still providing fast access to frequently accessed data.Auto tiering automatically manages data. It promotes data that becomes hot into DRAM and intelligently demotes unused data to SSDs. This opens new possibilities for applications that rely on large data collections.Fast data on large datasets is not the only benefit. Saving money is another advantage – and a reason that the finance department understands. In-memory storage can be expensive. By offloading less frequently accessed data to SSD, developers can optimize memory usage and reduce the costs associated with high-capacity memory requirements.Practically speaking, that makes data-heavy applications run faster without extra effort on the developer’s part. It also saves up to 70% in infrastructure costs, compared to deployments only using DRAM. And because auto tiering efficiently and automatically manages data access patterns, you don’t have to spend cycles (computing or human-brain-wise) identifying hot data versus warm data.To boost this feature, Redis forged a strategic partnership with Speedb, an innovative key-value storage engine. We integrate its technology as the default auto tiering engine.With the integration of Speedb, Redis Enterprise achieves a remarkable enhancement in performance, doubling the throughput and cutting latencies in half while using the same resources. This significantly widens the range of use cases that can leverage auto tiering’s benefits. Following this improvement, Redis Enterprise sizing for databases using Auto Tiering got increased to 10k ops/sec per core.Sure, we doubled the throughput, and we cut latencies in half, but numbers tell only part of the story. Examples matter.The following graphic shows a sample of the performance evolution of auto tiering in real workload scenarios. The blue bars represent Redis Enterprise 6.4 with the previous storage engine (RocksDB), and the red bars represent Redis Enterprise 7.2 with Speedb. For infrastructure, we used I4i.8xlarge AWS instances to host a 1TB database on 10 shards, replicated for high availability for a total of 20 shards, serving 1,024 clients.To simulate the most standard Redis’ use case, we defined two different payloads, 1KiB and 10KiB, over a configuration with 20% DRAM and 80% SSD with three possible use patterns, balanced read/write (1:1), heavy read (1:4), and heavy write (4:1). In both scenarios we measured the throughput in operations per second, and the corresponding latency. The following charts show the results.Compared to RS 6.4 (RocksDB), RS 7.2 (Speedb) improves:Compared to RS 6.4 (RocksDB), RS 7.2 (Speedb) improves:In all cases, Redis Enterprise 7.2 with Speedb has a better throughput, which means faster applications and less infrastructure needed to sustain this level of performance.Auto tiering is particularly applicable in scenarios that involve segregating data into hot data and warm data. One example would be banking applications that need to access up-to-the-minute data and historical data.Let us take a closer look at a mobile banking application example.Nowadays, everyone has a banking application on their mobile device. Users log into the application, get their balance, check the last transaction, and obtain other relatively small and focused information. Everyone expects this process to be smooth, simple, and instantaneous. That data is our hot data, which resides on DRAM in the Redis Enterprise database.Less frequently, users want additional information, such as a record of old transactions–perhaps a tax document from two years ago. It needs to be accessible but data access speed is less critical. This dataset is our warm data and can be kept in SSDs.Speed matters in other industries, too. For instance, gaming applications have strict latency requirements. Plus, by their nature, games are trendy. Over time, a gaming company accumulates user data, which is stored in a profiles database. But not all users are active users. With auto tiering, the active users’ profile information can reside on DRAM, while information about the rest of the users resides on the SSD.Our auto tiering product page goes into more technical details. Or if you’re ready to use it today, you can try Redis Enterprise 7.2 with auto tiering by creating a database on Redis Enterprise Cloud in the free or flexible plans or deploying a self-managed instance from our download center and following the auto tiering configuration guide."
8,https://redis.com/blog/introducing-redis-data-integration/,Introducing Redis Data Integration,"August 15, 2023",Yaron Parasol,"Redis is announcing the public preview release of Redis Data Integration (RDI). RDI lets developers offload a database to Redis Enterprise, mirror application data, and operate at in-memory speeds. And you don’t need to invest in coding or integration efforts.The underlying problem: Your existing database is too slow.You have a lot of applications, a growing number of users, increasing technical demands, and an unrelenting demand for real-time response. Redis Enterprise provides real-time access to data and it scales horizontally, but how do you keep your Redis cache in line with your database so that all queries can be executed from the cache?Some organizations decide to take it on themselves – only to discover how hard it is to build a cache prefetch (or refresh ahead, as it is sometimes called). To create one, you need to build a reliable streaming pipeline. That starts with capturing all data changes in the source database as they occur, and then translating the data to Redis data types to allow an application to fetch it. This process typically involves data transformations and denormalizations.We saw users struggling to build these streaming pipelines on their own. It required integration of several components (Change Data Capture (CDC), streaming, and Redis connectors), coding transformations, error handling, and many other enterprise essential requirements. That tool-building time could be spent on more productive endeavors that the business is waiting for.We decided to take on the challenge ourselves.Redis Data Integration (RDI) is a tool that runs inside Redis Enterprise. It helps you synchronize data from your existing relational database into Redis in near real-time so that application read queries are completely offloaded from the relational database to Redis.RDI pipelines have two stages:The data transformation processDebezium, an open-source CDC platform, captures changes to data in the source database and streams it into RDI. Within Redis, the data may be further filtered, transformed, and mapped to one or more Redis keys. RDI supports several Redis data types (Hash, JSON, Set, and Stream). RDI writes the data to the destination Redis database.It does the heavy lifting, so developers can focus on application code instead of on integration chores and data transformation code.RDI can connect with other CDC tools and streaming data. We already started building our ecosystem, and are happy to share that we have a technology partnership with Arcion.With this integrated solution, developers have a simple way to stream changes from a variety of databases to Redis Enterprise and other data platforms, using RDI as the backbone.Capturing changes from a source database and getting the data from one place to another is difficult enough. However, there is yet another challenge in moving data: the transformation part, which means filtering data and mapping the data to Redis data models.RDI provides an option to specify all the filtering and transformation steps required per source table. This is called a job, in RDI terms; every job is a YAML file.Filtering is important. CDC products provide complex filtering, but you have to write custom code. RDI does the same without coding. Instead, a declarative filter using SQL expressions or Jmespath functions is applied. RDI comes with additional custom Jmespath functions for the convenience of the job creator.RDI has several levels of data transformation:RDI includes a Trace tool that helps you create and troubleshoot complex data pipelines without writing custom code. That speeds up the process and reduces the required efforts and skill set.After troubleshooting, amending the pipeline is done by a simple deploy command with no downtime.Additional features in the public preview:Learn more in our Redis Data Integration documentation.RDI is ideally suited for applications that meet the following criteria:This is a public preview. We are seeing RDI Ingest flow to general availability, we are working on features to integrate Redis in the opposite direction: to apply changes to Redis data to downstream databases.RDI is currently available only for self-managed Redis Enterprise clusters.If you are an existing customer of Redis Enterprise, download the RDI CLI package and follow the steps in the quick start guide. The installation guide walks you through installing and configuring the Debezium server. After you run a handful of RDI CLI commands, your pipeline will begin moving data from your source database to Redis.If you are not an existing customer of Redis Enterprise, you need to first install Redis Enterprise Software for Kubernetes. Then download the RDI CLI package and follow the steps in the quick start guide."
9,https://redis.com/blog/five-official-redis-clients/,Five New Official Redis Clients,"August 15, 2023",Cody Henshaw,"Redis is committed to making using our software a delight to use. Adding these five new clients makes it even easier to do so.Our starting point in encouraging Redis use is making it easy for any developer to adopt. If you can get underway without having changing from familiar languages, tools, and application development platforms, you’re more likely to explore the powerful features we’re so proud of. Redis Stack supports a lot of functionality, with the latest supported protocols, embedded security, and performance that makes you gasp, “Wow, that’s fast!”Redis now officially supports five open-source client libraries:More are on the way.These client libraries seamlessly integrate into your applications. That means you can put your attention on creating awesome applications, instead of on debugging and patching third-party clients.We are dedicated to keeping the client libraries updated with the latest functionality and optimizations that Redis Stack has to offer. Whichever Redis version you rely on, you can count on seamless migration and upgrades. We will provide a standard interface across all Redis flavors, whether you use Redis Open Source (Redis OSS), Redis Enterprise Cloud, Redis Enterprise Software, or Redis on Kubernetes.By choosing our official Redis clients, you get:As part of our commitment to support developers, the officially supported libraries offer:Choosing one of our official Redis clients means prioritizing your development experience. You and your teams can work on the fun stuff–building incredible applications–while we handle the complexities of client libraries.To get underway, consult one of our Quick Start guides:"
10,https://redis.com/blog/understanding-redis-for-cloud-and-multicloud-in-90-seconds/,Understanding Redis for Cloud and Multicloud in 90 Seconds,"December 23, 2021",Will Johnston,"Welcome back to our ongoing Redis in 90 seconds series. In this post, we’ll demonstrate how to use Redis with any major cloud providers, or in a hybrid cloud.Most Redis providers simply host open source Redis and provide Redis as a cache. They don’t support Redis as a database. Not only that, they tend to lock your app into their cloud.If you need to move to a different cloud provider for any reason, you’re often restricted. It’s because the rest of your data is stored in cloud-specific databases and services such as DynamoDB, Kinesis, etc. If you use multiple clouds at the same time in place of a multicloud, often because of constraints on the region’s availability, or specific business needs, you can’t easily do that. Lastly, if you need a hybrid cloud capability to store some private data on-prem while still using the cloud for the rest, you’re in for a lot of trouble. When you use other Redis providers you lose all the flexibility and you’re “cloud locked-in”.With Redis Enterprise, since all or most of your data is stored in a single system, you can easily move from one cloud to another. Redis Enterprise is available on all major cloud providers, such as Amazon AWS, Google Cloud, Microsoft Azure, and even Heroku, making multicloud deployments a breeze. Redis Enterprise is available as downloadable software. Keep your private data in your private data center and keep the rest in the cloud, using Redis Enterprise’s hybrid deployment.Watch the video below to see what we mean:"
11,https://redis.com/blog/lowering-costs-with-redis-enterprise-in-90-seconds/,Lowering Costs with Redis Enterprise  in 90 Seconds,"January 4, 2022",Will Johnston,"We have reached the final post in our “Redis in 90 seconds” series. The purpose of this series is not to cover every aspect of Redis in-depth. Instead, we want to briefly highlight a few topics that you might not already know about Redis. In this post, we’ll walk you through what you can do to start lowering costs with Redis Enterprise.Have you ever compared Redis with other Redis providers and been surprised at the cost? It is a common question we are asked all the time. These providers allow you to use Redis as a cache and get the performance gains with sub-millisecond queries. The problem is, other Redis providers don’t optimize Redis to minimize your costs.Even database solutions such as DynamoDB can be expensive and slower than Redis, with queries upwards of 10 milliseconds or more.Redis Enterprise, even with all the added features, starts at just $7/mo and stays as low as $0 for your first six months when you use credits. This is great for small and medium-sized businesses (SMBs), and unlike other providers, Redis Enterprise is optimized for performance and cost savings at scale.With Redis Enterprise, take advantage of Redis on Flash to store terabytes of data while achieving sub-millisecond latencies and keeping costs at a minimum. This results in up to 80% in savings versus Redis from other cloud providers. So, Redis Enterprise isn’t only feature-rich and blazing fast, it’s also cost-effective for both SMBs and large enterprises.Watch the video below to see how you too can start lowering costs with Redis Enterprise"
12,https://redis.com/blog/scale-redis-across-the-globe/,Learn How to Scale Redis  Across the Globe in 90 Seconds,"December 28, 2021",Will Johnston,"We love technical articles that go deep. However, sometimes you’re on the go and need a quick summary to get the gist of something. This is why we’re continuing our “Redis in 90 Seconds” series with a short post about how to scale Redis.In today’s world, enterprise workloads can be very demanding. Enterprise customers demand low latency and fast failover with no data loss. Enterprise applications need to be distributed globally while minimizing latency.You might think throwing Redis on top of your existing database might solve this problem. Unfortunately, it only solves part of the problem. This is because most Redis providers simply host Redis OSS. They provide Redis as a cache, but not as an enterprise-grade real-time database.Redis Enterprise provides 99.999% uptime, sub-millisecond latency, single-digit-seconds failover, Active-Active Geo-Replication, and no data loss. It’s the only Redis provider that can meet the demands of enterprise customers.For example, with Redis Enterprise you can have multiple primaries spread across the globe and provide local, sub-millisecond latencies for both reads and writes. In order to avoid any write conflicts, Redis Enterprise uses the cutting edge Active-Active Geo-Replication feature that’s based on conflict-free replicated data types (CRDTs). Similarly, Redis Enterprise provides enterprise clustering, Redis on Flash, and many other native functions that address the enterprise solutions you won’t find anywhere else.Watch the video below to see Redis Enterprise in action:"
13,https://redis.com/blog/redis-cache-vs-redis-primary-database-in-90-seconds/,Redis as a Cache vs Redis as a Primary Database in 90 Seconds,"August 8, 2022",Will Johnston,"We received a lot of good feedback on our post titled, “Learn How Redis Simplifies Your Architecture in 90 Seconds,” so we decided to do a follow-up about Redis as a cache versus Redis as both a cache and a primary database.Get Started With Redis Cloud: Try FreeRedis began as a caching database, but it has since evolved into a primary database. Many applications built today use Redis as a primary database.  However, most Redis service providers support Redis as a cache but not as a primary database. This means you need a separate database like DynamoDB in addition to using Redis. This adds complexity, compromises latency, and prevents you from realizing the full potential of Redis.StackOverflow voted Redis the Most Loved Database three years in a row, and more than 2 billion Redis Docker containers have been launched. Knowing this, it shouldn’t be hard to find Redis expertise. And when Redis developers get stuck, there are literally thousands of resource books, tutorials, blog posts, and more to help resolve the issues.There are hundreds of Redis client libraries covering every major programming language and even some obscure ones. In many languages, developers can choose from various libraries to get just the right style and abstraction level. Redis is a database for a range of data sizes, from a few megabytes to hundreds of terabytes.With Redis Enterprise, you can use Redis as both an in-memory cache and a primary database in a single system, thus eliminating the complexity and latency of two separate systems. Not only that, you can use it as a multi-model primary database, enabling you to build modern applications, as well as low-latency microservice-based architectures, all on top of Redis.Instead of relying on separate databases and caches, utilize the native features of Redis Enterprise, such asLeverage all of the above with auto-scaling, enterprise clustering, and Active-Active Geo-Distribution.Watch the video below to see what we mean:Scaling both a cache and a database is often complicated; each data layer scales differently, reaching infrastructure and optimization opportunities at different times. Additionally, reducing the number of moving parts reduces latency; even though any given piece of the architecture may be fast, each item adds some sort of latency, either through the database itself or the connections made between items. Going to a single data store eliminates several internal network traversals. Finally, developing applications with a single data store requires only a single programmatic interface. Hence, developers need only understand the intricacies of a single database rather than a database and a cache. That reduces the mental cost of context-switching during development.Redis Enterprise’s Active-Active deployments are integral to achieving that 99.999% reliability and global scalability. This means a single dataset can be replicated to many clusters spread across wide geographic areas, with each cluster remaining fully able to accept reads and writes.Redis Enterprise uses Conflict-Free Replicated Data Types (CRDTs) to automatically resolve any conflicts at the database level and without data loss. Spreading clusters widely keeps data available at a geo-local latency and adds resiliency to cope with even catastrophic infrastructure failures.Many applications have relatively simple data needs, which can easily be supported via Redis’ built-in data structures. Other applications may need a bit more. For them, Redis provides an extensible engine that allows modules to add just the capabilities needed and no more. This approach extends to durability – Redis gives you the option of being entirely ephemeral, achieving durability through periodic snapshotting, or going all the way up to on-write durability with Append-only File (AOF). Redis can make the optimal trade-off between performance and durability depending on your use case.BSD-licensed and relatively compact, Redis is often cited as an example of a clean, well-organized C codebase. If something doesn’t make sense, it’s easy to find out and understand the absolute truth of what the database is doing. Nothing Redis does is magic – it’s just using long-established, efficient patterns to implement fundamental data structures.Performance expectations are a mainstay and are only getting more stringent with time. You’ll never hear a business leader say, “I wish our database was slower.” Thinking about building modern applications involves making them real-time, easy to develop, operationally elegant, scalable, and future-proof.Sure, Redis makes a great database cache but expanding Redis’ role as a primary database gives developers a head start on building the applications of tomorrow."
14,https://redis.com/blog/redis-enterprise-free-trial-aws-marketplace/,Introducing Redis Enterprise Cloud Free Trial on AWS Marketplace,"August 21, 2023",Sowmya Narayanan,"Redis is offering a new 14-day free trial on AWS Marketplace.For two full weeks, you can explore the full power of Redis Enterprise Cloud with no upfront costs or commitments. That gives you an opportunity to try Redis for a variety of use cases. Among them: caching, real-time analytics, publish/subscribe messaging, geo-location support, vector database analysis, and a whole lot more.Redis Enterprise Cloud is the only option on AWS that delivers sub-millisecond speeds, provides 99.999% uptime, and saves up to 80% on infrastructure costs (compared to other Redis-compatible managed services). Our partnership helps existing AWS users access Redis Enterprise Cloud’s simplified and automated database provisioning, billed directly from their AWS accounts.With Redis Enterprise Cloud, you get:Redis Enterprise Cloud is the most performant, reliable, and easy to use version of Redis, and getting started on AWS is easy and efficient. No matter where you are in your cloud journey, there’s an option to meet your needs. Hybrid? Multi-cloud? No problem.The free trial lasts for a 14-day period, OR when you use up to $500 in database subscriptions, whichever comes first. During the free trial, you can access Redis Enterprise Cloud with full features, including product support access. Consult the step-by-step instructions to learn what’s involved, and sign up through the AWS Marketplace.At the end of the trial period, you will automatically be subscribed as a Redis Enterprise Cloud pay-as-you-go customer, unless you cancel your subscription first.If you have any questions during the trial or need sign-up assistance, email aws@redis.com to connect with a Redis expert.New Redis Enterprise Cloud customers qualify for the 14-day free trial.If you are currently a Redis Enterprise Cloud customer, you may qualify for a Marketplace credit or proof of concept program. Fill out the form to get assistance from a Redis expert to learn about your options.Learn more about the free trial. Or just start your free trial today."
15,https://redis.com/blog/building-llm-applications-with-redis-on-googles-vertex-ai-platform/,Building LLM Applications with Redis on Google’s Vertex AI Platform,"August 24, 2023",Tyler Hutcherson,"Google’s Vertex AI platform recently integrated generative AI capabilities, including the PaLM 2 chat model and an in-console generative AI studio. Here, you learn about a novel reference architecture and how to get the most from these tools with your existing Redis investment.Generative AI, a fast-growing AI subset, has captured significant attention for its potential to transform entire industries. Google’s Google Cloud Platform (GCP) has been making strides to democratize access to generative AI to make adoption easier, and the company is backing it with robust security, data governance, and scalability.Recently, Google announced generative AI support on Vertex AI with four new foundation models:An application’s ability to produce, comprehend, and engage with human language is becoming essential. The need for this functionality spans numerous domains, from customer service chatbots and virtual assistants to content generation. And it is achievable thanks to foundation models, such as Google’s PaLM 2, that were meticulously trained to generate human-like text.Amidst this dynamic environment, two fundamental components consistently stand out as critical for the creation of efficient, scalable language model applications: foundation models and a high-performance data layer.Foundation models, of which large language models (LLMs) are a subset, are the cornerstone of generative AI applications. LLMs are trained on vast collections of text, which enables them to create contextually relevant, human-like text for an array of tasks. Improvements in these models have made them more sophisticated, leading to more refined and effective responses to user inputs. The chosen language model significantly impacts an application’s performance, cost, and overall quality.Yet, for all their capabilities, models such as PaLM 2 have limitations. When they lack domain-specific data, the models’ relevance can wane, and they might lag in reflecting fresh or correct information.There are hard limits to the context length (i.e number of tokens) that LLMs can handle in prompts. Plus, LLM training or fine-tuning requires substantial computational resources that add considerable cost.Balancing these limitations with the potential benefits requires careful strategy and a robust infrastructure.An efficient LLM application is underpinned by a scalable, high-performance data layer. This component ensures high-speed transactions with low latency, which is crucial for maintaining fluid user interactions. It plays a vital role in caching pre-computed responses or embeddings, storing a history of past interactions (persistence), and conducting semantic searches to retrieve relevant context or knowledge.Vector databases have emerged as one popular solution for the data layer.  Redis invested in vector search well before the current wave, and the technology reflects our experience–particularly with performance considerations. That experience is reflected in the just-announced Redis 7.2 release, which includes a preview of scalable search features that improves queries per second by 16X, compared to the previous version.Foundation models and vector databases have sparked substantial interest (and hype) in the industry, given their pivotal role in molding LLM applications across different sectors. For example, some newer standalone vector database solutions, such as Pinecone, announced strong funding rounds, and are investing a lot of effort into gaining developer attention. Yet, with all of the new tools emerging each week, it’s hard to know where to put your trust to be enterprise-ready.What sets GCP apart is its unified offering. It marries the availability of powerful foundation models with scalable infrastructure and a suite of tools for tuning, deploying, and maintaining these models. Google Cloud places paramount importance on reliability, responsibility, and robust data governance, ensuring the highest level of data security and privacy. Guided by its publicly declared AI principles, GCP champions beneficial use, user safety, and ethical data management, providing a dependable and trustworthy application foundation.However, to truly tap into these advancements, a complementary high-performing and scalable data layer is indispensable.Naturally, this is where Redis steps in. In the subsequent sections, we dissect these core components and explore their interaction through a reference architecture.The reference architecture illustrated here is for general-purpose LLM use cases. It uses a combination of Vertex AI (PaLM 2 foundation model), BigQuery, and Redis Enterprise.GCP and Redis Enterprise reference architecture for LLM applicationsYou can follow along with the setup of this LLM architecture step-by-step using the Colab notebook in an open-source GitHub repository.Once you complete the necessary setup steps, this architecture is primed to facilitate a multitude of LLM applications, such as chatbots and virtual shopping assistants.Even experienced software developers and application architects can get lost in this new knowledge domain. This short summary should bring you up to speed.Semantic search extracts semantically similar content from an expansive knowledge corpus. Doing so depends on the power of Natural Language Processing (NLP), foundation models like PaLM 2, and a vector database. In this process, knowledge is transformed into numerical embeddings that can be compared to find the most contextually relevant information to a user’s query.Redis, in its role as a high-performing vector database, excels at indexing unstructured data, which enables efficient and scalable semantic search. Redis can enhance an application’s capacity to swiftly comprehend and react to user queries. Its robust search index facilitates accurate, responsive user interactions.The Retrieval-Augmented Generation (RAG) approach uses methods like semantic search to dynamically infuse factual knowledge into a prompt before it’s sent to a LLM. The technique minimizes the need to fine-tune an LLM on proprietary or frequently changing data. Instead, RAG permits contextual augmentation of the LLM, equipping it to better tackle the task at hand, whether that’s answering a specific question, summarizing retrieved content, or generating fresh content. RAG is typically implemented within the scope of an agent.Agents involve an LLM making decisions about which actions to take, taking the stated action, making observations, and iterating until complete. LangChain provides a common interface for agent development.Redis, as a vector database and full text search engine, facilitates the smooth functioning of RAG workflows. Owing to its low-latency data retrieval capabilities, Redis is often a go-to tool for the job. It ensures that a language model receives the necessary context swiftly and accurately, promoting efficient AI agent task execution.Caching serves as a potent technique to enhance LLM responsiveness and computational efficiency.Standard caching provides a mechanism to store and quickly retrieve pregenerated responses for recurrent queries, thereby reducing computational load and response time. However, in a dynamic conversational context with human language, identically matching queries are rare. This is where semantic caching comes into play.Semantic caching understands and leverages queries’ underlying semantics. Semantic caching identifies and retrieves cached responses that are semantically similar enough to the input query. This ability dramatically increases the chances of cache hits, which further improves response times and resource utilization.For instance, in a customer service scenario, multiple users might ask similar frequently-asked-questions but use different phrasing. Semantic caching allows LLMs to respond swiftly and accurately to such queries without redundant computations.Redis is highly suited for facilitating caching in LLMs. Its robust feature set includes support for Time-To-Live (TTL) and eviction policies for managing ephemeral data. Coupled with its vector database capabilities for semantic searches, Redis enables efficient and rapid retrieval of cached responses, resulting in a noticeable boost in LLM response speed and overall system performance, even under heavy loads.It is important to retain past interactions and session metadata to ensure contextually coherent and personalized dialogues. However, LLMs do not possess adaptive memory. That makes it crucial to rely on a dependable system for swift conversational data storage.Redis offers a robust solution for managing LLM memory. It efficiently accesses chat history and session metadata, even under substantial demand. Using its data structure store, Redis handles traditional memory management, while its vector database features facilitate the extraction of semantically related interactions.What does it matter? Who needs these features? These three scenarios demonstrate the practical application of this LLM architecture.Some businesses need to handle vast volumes of documents, an LLM empowered application can serve as a potent tool for document discovery and retrieval. Semantic search aids in pinpointing pertinent information from an extensive corpus of knowledge.An LLM can serve as the backbone for a sophisticated ecommerce virtual shopping assistant. Using contextual understanding and semantic search, it can comprehend customer inquiries, offer personalized product recommendations, and even simulate conversational interactions, all in real time.Deploying an LLM as a customer service agent can revolutionize customer interactions. Beyond answering frequent queries, the system can engage in complex dialogue, providing bespoke assistance while learning from past interactions.A whirlwind of new products and buzzwords can easily lead to a shiny object syndrome. Amidst this cacophony, the combination of GCP and Redis shines through, providing not just an innovative solution but a reliable and time-tested foundation.Grounded in factuality: GCP and Redis empower LLM applications to be more than just advanced text generators. By swiftly injecting domain-specific facts from your own sources at runtime, they ensure your applications deliver factual, accurate, and valuable interactions that are specifically tailored to your organization’s knowledge base.Architectural simplification: Redis is not just a key-value database; it’s a real-time data Swiss army knife. By eliminating the need for managing multiple services for different use cases, it vastly simplifies your architecture. As a tool that many organizations already trust for caching and other needs, Redis’ integration in LLM applications feels like a seamless extension rather than a new adoption.Optimized for performance: Redis is synonymous with low-latency and high-throughput data structures. When coupled with GCP’s unrivaled computing prowess, you have an LLM application that not only talks smart but also responds swiftly, even under heavy demand scenarios.Enterprise readiness: Redis isn’t the new kid on the block. It’s a battle-tested, open-source database core, reliably serving Fortune 100 companies worldwide. With achievable five nines (99.999%) uptime in its enterprise offering and bolstered by GCP’s robust infrastructure, you can trust in a solution that’s ready to meet enterprise demand.Accelerate time to market: With Redis Enterprise available at your fingertips via the GCP Marketplace, you can put more focus on crafting your LLM applications and less on wrangling with setups. This ease of integration speeds time to market, giving your organization a competitive edge.While new vector databases and generative AI products might create a lot of noise in the market, the harmony of GCP and Redis sings a different tune–one of trust, reliability, and steadfast performance. These time-tested solutions aren’t going anywhere soon, and they’re ready to power your LLM applications today and for years to come.Redis will attend Google Next ‘23 between August 29-31 in San Francisco. We invite you to visit the Redis booth, #206, where we’re happy to talk about the details.Ready to get your hands dirty now? You can set up Redis Enterprise through the GCP Marketplace and work through the getting started tutorial."
16,https://redis.com/blog/understanding-topology-based-data-architectures/,Understanding Topology-Based Data Architectures,"August 28, 2023",Amine El Kouhen,"Data architectures can be classified based on their operational mode or topology, including data fabric, data hub, and data mesh. What’s the distinction between those? Buckle in.Previously, when I explained velocity-based data architectures, I went into detail about Lambda and Kappa architectures. But that’s just the beginning of the story.While data architectures can be grouped in relation to data velocity, as we’ve seen with Lambda and Kappa, another way to classify them is technology-agnostic operational models. As you surely remember if you have been following along with this Data Basics series, three types of operating models can exist in any organization: centralized, decentralized, and hybrid.Here, I describe three topology-based data architectures in more detail: data fabric, data mesh, and data hub.A data hub is an architecture for managing data in a centralized way. Think of it as a data exchange with frictionless data flow at its core.A data hub acts as a central repository of information with connections to other systems and customers, allowing data sharing between them. Endpoints interact with the data hub by providing data into it or receiving data from it. The hub provides a mediation and management point, making visible how data flows across the enterprise.A data hub architecture facilitates this exchange by connecting producers and consumers of data. The seminal work was a Gartner research paper, Implementing the Data Hub: Architecture and Technology Choices, published in 2017. Gartner suggested a technology-neutral architecture for connecting data producers and consumers, which was more advantageous than point-to-point alternatives. Subsequent research further developed this concept, resulting in the current definition of a data hub’s attributes.The hub is structured and consumed according to the models defined by its users. Governance policies are established by data managers to ensure data privacy, access control, security, retention, and secure information disposal. Developers can use integration strategies such as APIs or Extract-Transform-Load (ETL) processes to  work with the data stored within the hub. The persistence attribute defines which type of datastore should be used for storing this data (such as data lakes, data warehouses, or data lakehouses) and the different parameters of storage, such as the raw support, storage system, and storage layers.Implementing a data hub architecture facilitates:Gartner proposed that businesses could use specialized, purpose-built data hubs. For instance, analytics data hubs might collect and share information for downstream analytics processes. Or application data hubs could be used as domain context for specific applications or suites.Data centralization implemented in data hubs ensures that data is managed from a central source–with a unified vision–and it keeps the data accessible from many different points. The beneift is that it minimizes data silos, fosters collaboration, and provides visibility into emerging trends and impacts across the enterprise.However, there are challenges with data centralization. Processes can be slow without putting accelerators or some sort of self-service strategy in place. As a result, requests take longer and longer to get done. The business can’t move forward fast enough, and opportunities for better customer experiences and improved revenue are lost simply because they can’t be achieved quickly.That’s among the reasons Gartner recently updated its data hub concept to allow organizations to run multiple hubs in an interconnected way. This way, the data hub can take advantage of data centralization and leverage decentralization by giving the lines of business more responsibility and power.The most common data hub usage is data warehouses. A data warehouse is a central data hub used for reporting and analysis. Typically, data in a data warehouse is highly formatted and structured for analytics use cases. As a result, it’s among the oldest and most well-established data architectures.In 1989, Bill Inmon originated the notion of the data warehouse, which he described as “a subject-oriented, integrated, nonvolatile, and time-variant collection of data in support of management’s decisions.” Though the technical aspects of the data warehouse have evolved significantly, the original definition still holds its weight.Traditionally, a data warehouse pulls data from application systems by using ETL. The extraction phase pulls data from source systems. The transformation phase cleans and standardizes the data, organizing and imposing business logic in a highly modeled form.One ETL variation is Extract-Load-Transform (ELT). With the ELT mode in data warehouse architectures, data is moved more or less directly from production systems into a staging area in the data warehouse. In this context, staging indicates that the data is in a raw form. Rather than using an external system, transformations are handled directly in the data warehouse. Data is processed in batches, and transformed output is written into tables and views for analytics.When the Big Data era began, the data lake emerged as another centralized architecture. The idea was (and is) to create a central repository where all types of structured and unstructured data are stored without any strict structural constraints. The data lake was intended to empower businesses by providing unlimited data supply.The initial version of the data lake started with distributed systems like Hadoop (HDFS). As the cloud grew in popularity, these data lakes moved to cloud-based object storage, which could depend on extremely cheap storage costs and virtually limitless storage capacity. Instead of relying on a monolithic data warehouse where storage and compute are tightly coupled, the data lake stores an immense amount of data of any size and type.It got a lot of hype. But the first generation of data lakes–data lake 1.0–had significant drawbacks. The data lakes essentially turned into a dumping ground, creating terms such as “data swamp” and “dark data,” as many data projects failed to live up to their initial promise. Management became increasingly difficult as the data volume grew exponentially, and schema management, data cataloging, and discovery tools were lacking. In addition, the original data lake concept was essentially write-only, creating huge headaches with the arrival of regulations such as GDPR that required targeted deletion of user records. Processing data was also a major challenge, with relatively basic data transformations, such as joins, requiring the implementation of complex MapReduce jobs.Various industry players have sought to enhance the data lake concept to fully realize its promise and to respond to the first-generation data lake limitations. For example, Databricks introduced the notion of a data lakehouse, which suggests a convergence between data lakes and data warehouses. The lakehouse incorporates the controls, data management, and data structures found in a data warehouse while still housing data in object storage and supporting a variety of query and transformation engines. In particular, the data lakehouse supports atomicity, consistency, isolation, and durability (ACID) transactions. It is a significant disruption from the original data lake, where you simply pour in data and never update or delete it.Data decentralization is a data management approach that eliminates the need for a central repository by distributing data storage, cleaning, optimization, output, and consumption across organizational departments. This helps reduce complexity when dealing with large amounts of data and issues such as changing schema, downtime, upgrades, and backward compatibility.Data fabric was created first as a distributed data environment that enables the ingestion, transformation, management, storage, and access of data from various repositories for use cases such as business intelligence (BI) tools and operational applications. It provides an interconnected web-like layer to integrate data-related processes by leveraging continuous analytics over current and inferred metadata assets. To maximize efficiency, it uses techniques like active metadata management, semantic knowledge graphs, and embedded machine learning/AutoML capabilities.This concept was coined in 2016 by Noel Yuhanna of Forrester Research in Forrester Wave: Big Data Fabric, and has been updated since then. Yuhanna’s paper described a technology-oriented approach combining disparate data sources into one unified platform using Hadoop and Apache Spark for processing. The goal is to increase agility by creating an automated semantic layer. This later accelerates the delivery of data value while minimizing pipelines complexity.Over the years, Yuhanna developed his Big Data Fabric concept further. His current vision for data fabrics is for them solve a certain class of business needs, such as creating an all-encompassing view of customers, customer intelligence, and analytics related to the Internet of Things. The data fabric’s components include AI/ML, data catalog, data transformation, data preparation, data modeling, and data discovery. It also provides governance and modeling capabilities.Gartner also adopted the term “data fabric” and defined it similarly. The analyst firm describes it as “an emerging data management and integration design that enables flexible, reusable, and enhanced data integration pipelines, services, and semantics to support various operational or analytics use cases across multiple deployment platforms”.Data fabrics combine different data integration techniques while using active metadata, knowledge graphs, semantics, and machine learning (ML) to improve their design process. They organize them into five inner attributes.In a fabric, active metadata contains catalogs of passive data elements such as schemas, field types, data values, and knowledge graph relationships. The knowledge graph stores and visualizes the complex relationships between multiple data entities. It maintains data ontologies to help non-technical users interpret data.With the help of AI and ML features, a data fabric may assist and enhance data management activities. It also offers integration capabilities to dynamically ingest disparate data into the fabric to be stored, analyzed, and accessed. Automated data orchestration allows users to apply DataOps principles throughout the process for agile, reliable, and repeatable data pipelines.Data fabric is a technology-agnostic architecture. Its implementation enables you to scale Big Data operations for both batch processes and real-time streaming, providing consistent capabilities across cloud, hybrid multi-cloud, on-premises, and edge devices. It simplifies the flow of information between different environments so that a complete set of up-to-date data is available for analytics applications or business processes. And it reduces time and cost by offering pre-configured components and connectors, so nobody has to manually code each connection.Data mesh was introduced in 2019 by Zhamak Dehghani, who argued that a decentralized architecture was necessary due to shortcomings in centralized data warehouses and data lakes.A data mesh is a framework that enables business domains to own and operate their domain-specific data without the need for a centralized intermediary. It draws from distributed computing principles, where software components are shared among multiple computers running together as a system. In this way, data ownership is spread across business domains, each of which is responsible for creating its own products. A data mesh–in theory, anyway–allows easier contextualization of the collected information to generate deeper insights while simultaneously facilitating collaboration between domain owners to create tailored solutions according to specific needs.Dehghani later revised her position to propose four principles that form this new paradigm: domain-oriented, data-as-product, self-service, and federated governance.The data mesh concept is based on decentralizing and distributing responsibility for analytical data, its metadata, and the computation necessary to serve it to people closest to the data. This allows for continuous change and scalability in an organization’s data ecosystem.To do this, data meshes decompose components along organizational units or business domains that localize changes or evolution within that bounded context. By doing so, ownership of these components can be distributed across stakeholders close to the data.One issue with existing analytical data architectures is that it can be difficult and expensive to discover, understand, trust, and use quality data. The problem only gets worse as more teams provide data (domains) in a decentralized manner, which would violate the first principle.To address these challenges related to data quality and silos, a data mesh must treat analytical data provided by domains as a product and treat the consumers of that product as customers. The product becomes the new unit of architecture that should be built, deployed, and maintained as a single quantum. It ensures that data consumers can easily discover, understand, and securely use high-quality data across many domains.The infrastructure platform allows domain teams to autonomously create and consume data products without worrying about the underlying complexity of the building, executing, and maintaining secure and interoperable solutions. A self-service infrastructure should provide a streamlined experience that enables data domain owners to focus on core objectives instead of worrying about technical details.The self-serve platform capabilities fall into multiple categories or planes:Data mesh implementation requires a governance model that supports decentralization and domain self-sovereignty, interoperability through a dynamic topology, and automated execution of decisions by the platform. Integrating global governance and interoperability standards into the mesh ecosystem allows data consumers to gain value from combining and comparing different data products within the same system.The data mesh combines these principles into a unified, decentralized, and distributed system. The premise is that data product owners have a self-service, shared infrastructure that supports data-sharing pipelines that work in an open yet governed manner. This allows the developers to be productive without sacrificing governance or control over their domain’s data assets.A data mesh differs from traditional approaches, where  pipelines and data are managed as separate entities with shared storage infrastructure. Instead, it views all components (i.e., pipelines, data, and storage infrastructure) at the granularity of a bounded context within a given domain to create an integrated product. This allows for greater flexibility in terms of scalability and customization while providing better visibility into how different parts interact.How do we put all that together? Data types are increasing in number, usage patterns have grown significantly, and a renewed emphasis has been placed on building pipelines with Lambda and Kappa architectures in the form of data hubs or fabrics. Whether grouped by velocity or the kind of topology they provide, data architectures are not orthogonal. The data architectures and paradigms I described in these blog posts, so far, can be used as appropriate for a given need. And, of course, they can be mixed in architectures like data mesh, in which each data product is a standalone artifact. We can imagine scenarios where a Lambda architecture is implemented in some data products and Kappa architectures are employed in others.Want more detail? Our white paper, Best Practices for a Modern Data Layer in Financial Services, discusses the steps to modernize a rigid and slow IT legacy system."
17,https://redis.com/blog/cache-eviction-strategies/,Cache Eviction Strategies Every Redis Developer Should Know,"August 30, 2023",John Noonan,"Here’s a real-world scenario. You set up a Redis database, and it’s doing wonders at speeding up your application. But as the data flows in and the volume increases, you notice a potential issue: the cache is filling up. What will happen when it’s full? You may have heard about cache eviction but, perhaps, you’re fuzzy on the details.You’re not alone. Whether you’re a developer in a budding company or a system administrator in a large corporation, it’s important to understand cache eviction and know when and how to implement it. In this guide, we explore why.Cache eviction policies are a critical aspect of cache management when you use Redis (or any system that relies on caching… but we speak here from our own expertise). It addresses the challenges of cache size and memory usage. As the cache reaches its limits, it must make a crucial decision: should new data be rejected, or should space be created by discarding old data?This is where cache eviction comes into play. However you resolve it, cache eviction involves determining which cache entries to retain and which to discard when a cache fills up. It’s necessary to achieve or maintain optimal application performance and consistency.Eviction, in the context of caching, does not involve any landlords or overdue rents. Rather, it refers to the process of removing specific data from a cache. Because when a cache reaches its maximum storage capacity, some data must be removed to make space for new data–just like a bookshelf where you cannot force another book into the space available.Cache eviction strategies are protocols that dictate how a system responds when the cache is full. The decision about which data to evict (that is, remove) is made programmatically based on one of several strategies. Common strategies include:The effectiveness of these strategies depends on the specific use case.While Redis does have a default eviction policy (volatile-LRU), relying solely on it without understanding its implications can be risky. Applications serve diverse user needs, which means that data patterns and eviction requirements can vary significantly. Setting the right eviction policy can prevent potential headaches.Before even thinking of eviction, it’s essential to know when to act. This is where monitoring tools come into play.Redis provides tools like the INFO command for monitoring cache performance, while third-party monitoring tools like New Relic and Datadog offer more detailed analysis.Tuning cache performance involves adjusting cache settings and eviction policies based on the information you discover from monitoring performance. In distributed caching scenarios, monitoring and tuning become even more critical to ensure consistent and efficient cache management across multiple nodes. (We have additional advice for what to look for in a third-party monitoring tool.)In Redis, the cache entry data structure is managed by the maxmemory configuration directive, which sets the memory limit. The maxmemory-policy configuration directive guides Redis in making its eviction decisions based on the chosen cache eviction policy. Both the maxmemory-policy and the eviction policy are among the configuration settings stored in the redis.conf configuration file.There are quite a few Redis eviction policies, but you probably care most about these.Removes the least recently used cache entries, whether or not they have an expiration time set.Removes the least recently used cache entries with an expiration time set. This is suitable for scenarios where data needs to be refreshed periodically.When Redis needs to make room for new data, this policy removes the least frequently used keys.Similar to allkeys-lfu, this policy applies only to keys with an expiration time set.This policy removes keys with the shortest TTL first.Instead of evicting any keys, this policy returns an error when the memory limit is reached and a write command is received. (Don’t throw out anything in the closet. Send an alarm!)Each policy has its strengths and weaknesses. The best one for you depends on your specific needs.It’s important to have a well-structured cache, combined with the right cache eviction policy in order to achieve performance goals when you have vast amounts of data. Redis, with its versatile capabilities, serves as an excellent caching solution and a powerful asset for applications handling large datasets. Effective cache management not only expedites data retrieval through cache hits but also mitigates the impact of cache misses, making Redis a reliable and efficient caching solution for diverse use cases.Discover the intricacies of scaling cache with our comprehensive guide: The Definitive Guide to Caching at Scale with Redis. Learn the basics of caching to advanced enterprise application techniques in this one-stop resource."
18,https://redis.com/blog/introducing-redis-7-2/,Redis 7.2 Sets New Experience Standards Across Redis Products,"August 15, 2023",Rowan Trollope,"You have trusted Redis for over a decade because we make it easy to create powerful, fast applications that perform at scale––and we try hard to deserve that reputation. Redis is continuing that spirit with all the innovation we put into Redis 7.2.Here’s what we are doing to increase the number of reasons you love us.There’s an irreplaceable sort of learning that can only be gained by using a tool you’re leading the development of. Before I took up the mantle of Redis CEO, I dove headfirst into using the product as a developer. Its unparalleled performance, its scalability, and its design approach (as outlined in the Redis manifesto) inspired me, just as it has inspired millions of other developers. I saw why Redis is one of the most successful open-source databases in the world.Among the reasons for Redis’ growth are increasing demands for faster applications, real-time inferencing, and the emerging broad adoption of generative AI and vector databases. As Redis has expanded far beyond its beginnings as a data structure server (often used for caching), new waves of developers want to enjoy Redis’ benefits. We realized that we need to expand our responsibility for the full experience for all Redis practitioners: developers, architects and operators.As a developer, I appreciated how elegant and easy it is to use the Redis core API. So much so, in fact, that I wanted that delightful experience to be reflected in other parts of the product portfolio, such as in clients, integrations, tools, and documentation. We need to address this, I decided––and to take a more active guiding role.I joined Redis in February. Since then, we’ve established a strategy to expand our role as the stewards of the Redis project. In that role, we embrace Redis’s community-driven beauty, while making all distributions of Redis easier to navigate, with clear directions and well-marked trails. By acting as community stewards, we help both newcomers and long-time Redis practitioners enjoy Redis’ uncompromising performance, legendary reliability, and simplicity.Redis has always been a formidable contender in the tech industry, but the journey to becoming the best version of oneself is never-ending. We’ve been hard at work to make Redis even more rewarding for developers, architects, and operators.Today, I’m excited to introduce Redis 7.2, a step forward in our continuing journey to refine and improve your experience.So, let’s dive in and explore the many ways we are making technology easier for Redis users. Because at Redis, we’re committed to helping everyone build better software, faster, and with more confidence.Redis 7.2 is our most far-reaching release. It encompasses a broad set of new features, and a significant investment in the functionality that supports AI initiatives. In each of these enhancements, you’ll notice a strong theme of making it easier for developers to use Redis, making it run even faster, and making it easier to achieve innovative results.And we are committing to deliver all of these capabilities through every distribution channel all at once, with an approach we call the Unified Redis Release.The effort to leverage large language models (LLM) and generative AI is transforming computer software at a blistering pace, and we’ve been hard at work delivering capabilities in our platform to make that effort an easy one. We’ve been powering some of the largest customers in the world (including OpenAI) and have years of investment in making machine learning (ML)–and now vector databases–seamless and easily accessible.We also grok the concerns of the businesses that are doing their best to innovate based on them. For instance, one of my first customer visits was to a large financial services customer. The company has multiple ML workloads running exclusively on Redis, with hundreds of terabytes running at 5 9’s of availability. Enterprises are looking for a vector database that is a proven, enterprise-grade database with Active-Active geo-distribution, multi-tenancy, tag-based hybrid search, role-based access, embedded objects (i.e. JSON), text-search features, and index aliasing. We have all those built-in and battle-tested on Redis Enterprise.Redis supports generative AI workloads in its database service through several strategies that aim to improve efficiency, reduce costs, and enhance scalability and performance. Redis’ vector database supports two vector index types: FLAT (brute force search) and HNSW (approximate search), as well as three popular distance metrics: Cosine, Inner Product, and Euclidean distance. Other features include range queries, hybrid search (combining filters and semantic search), JSON objects support, and more.But, when people ask us how Redis can help with building and deploying LLM-powered apps? What can we really do?In the past 12 months, we’ve integrated Redis with the most popular application development frameworks for creating LLM-powered chatbots, agents, and chains. Among these are LlamaIndex, Langchain, RelevanceAI, DocArray, MantiumAI, and the ChatGPT retrieval plugin.  In addition, we’ve worked closely with NVIDIA on some of its leading AI projects: NVIDIA’s AI Workflows (Merlin and Morpheus), Tools (Triton and RAPIDS), and then in the works is RAPIDS RAFT–state of the art indexing provided by NVIDIA to deliver higher queries per second (QPS).These use cases require a new level of high-performance search. With Redis Enterprise 7.2, we’re introducing a preview of scalable search capability. It allows running high-QPS, low-latency workloads with optimal distributed processing across clusters. It can improve query throughput by up to 16X compared to what was previously possible with Redis Enterprise’s search and query engine.You don’t become the most admired NoSQL database (according to the Stack Overflow 2023 survey) without some serious developer focus.Redis 7.2 addresses one area that frustrated me at the beginning of my Redis developer journey, which was figuring out which of the hundred-plus community-developed client libraries fit my needs. Which one supports the latest Redis functionality? The right level of security and performance?With Redis 7.2, we bring a new level of guidance and support to Redis clients. We are working directly with the community maintainers of five client libraries––Jedis (Java), node-redis (NodeJS), redis-py (Python), NRedisStack (.Net), and Go-Redis (Go)––to establish consistency in such things as documentation, user interface, governance, and security. We also support the RESP3 protocol in Redis Stack and Redis Enterprise (cloud and software).With this release, we also bring a new level of programmability for real-time data. The public preview of Triggers and Functions brings a server-side event-driven engine to execute Typescript/JavaScript code within the database. This feature lets developers perform complex data manipulations directly on Redis, ensuring consistency of execution across any client application.Triggers and functions enable cross-shard read operations at the cluster level. This functionality was not available in previous generations of the Redis programmability engine, such as Lua and functions.Geospatial features are improved, too. We improved polygon search in Redis Stack to facilitate the search of geospatial data to find information within a geographic area.For example, in an application to locate taco restaurants, the geospatial information is the indexed location data of all the restaurants in San Francisco. The polygon that a user draws on a digital map is the geographic area of the search. Redis retrieves only the keys associated with the restaurants within the boundaries of the drawn polygon.We also made significant performance improvements in the Redis data type of sorted sets, commonly used to create gaming leaderboards among its other uses. Our enhancements generate gains between 30% and 100% compared to Redis Enterprise Cloud 6.2.You can now use Redis Data Integration (RDI), a tool that runs directly on Redis Enterprise and effortlessly transforms any dataset to Redis. We captured the most common use cases and made them available through an interface with configuration, not code.RDI can take data from a variety of sources (such as Oracle, Postgres, or Cassandra) and functionally turn it into real-time data. Similarly, when the data is no longer “real-time,” RDI can bring downstream changes from Redis Enterprise into the system of record without having to add more code or perform arcane integrations.RDI (currently in public preview) streams changes from source databases directly into Redis, where they are further filtered, transformed, and mapped into formats such as JSON and Hash.Your applications don’t always require top speed for every use case. Sometimes you don’t need to store all the data in memory. It makes more sense–and saves money–to leverage lower cost storage like SSD.With Redis Enterprise 7.2, we introduce Auto Tiering (formerly called Redis on Flash) with a new default storage engine, Speedb. Auto tiering allows operators to extend the size of Redis databases beyond the limits of physical DRAM using solid-state drives (SSDs). This makes sense for applications with large datasets where heavily used data stays in memory and, maintaining less frequently used data in SSD. Redis Enterprise automatically manages memory based on usage.Auto tiering delivers significant performance improvements in terms of throughput and latency, doubling throughput at half the latency of the previous generation storage engine (RocksDB), and reducing infrastructure costs by up to 70%.There are two other innovations in this release that we want to highlight. Operators will find the updated cluster manager (CM) user interface of great help to speed up everyday administrative tasks. Its intuitive interface has features comparable to those of the Redis Enterprise Cloud that reduces learning curves and minimizes errors. For example, with the new CM, you can deploy a new Redis Enterprise database with only two mouse clicks.The other piece of good news that surely will please operators of containerized applications is the general availability of the Redis Enterprise Operator for Kubernetes with support for Active-Active database deployments. With a few declarative lines in a YAML file, you can simplify several tasks that take time and effort, such as creating geo-distributed databases and removing, adding, and updating participating clusters from an Active-Active database.Many developers prefer to start building on a Redis service in the cloud rather than download it to their machines. We’ve also heard from many customers about the challenges of dealing with separate delivery dates of new Redis releases for their mix of Redis OSS, Redis Stack, and Redis Enterprise instances.Redis 7.2 is our first Unified Redis Release, and it is generally available today. We are making it easier for developers to build and port code between different Redis distributions. This is a significant boon for operators who want to streamline control over their Redis footprint. Architects will appreciate the freedom to integrate other data stores with Redis.Our commitment to the Redis community and customers is to release all Redis products and distributions at the same time. This includes Redis OSS, Redis Stack, Redis Enterprise Cloud, Redis Enterprise Software, and Redis Enterprise on Kubernetes.Redis 7.2 signals our unwavering commitment to make it easier for all of you, Redis lovers, to get started building with Redis, to more easily and cost-effectively run Redis at large scales, and to bring slow data into Redis and make it actionable in real time.The AI revolution is here, and I cannot overstate its far-reaching effects on our work and lives in general. Our focus on our vector database and vector similarity search, make it a perfect match in helping you harness your Redis skills to jumpstart your AI projects.We invite you to read more details on the new features in Redis Enterprise 7.2 with the specifics of Auto Tiering, Triggers and Functions, and Redis Data Integration.Are you ready to try it for yourself? The easiest way is to use Redis Enterprise Cloud, create a free account to try the latest features using Redis Stack, or download the software for a self-managed or Kubernetes deployment.For the features in public preview, such as triggers and functions, deploy a database on Redis Enterprise Cloud in the fixed tier within the Google Cloud/Asia Pacific (Tokyo) or AWS/Asia Pacific (Singapore) region. For a self-managed experience, visit our download center."
19,https://redis.com/blog/redis-observability-with-uptrace/,Enhancing Redis Observability with Uptrace,"July 19, 2023",Talon Miller,"Everyone needs a way to monitor server behavior, if only to confirm that the system is running to spec. Several Application Performance Monitoring (APM) tools work with Redis, but perhaps you haven’t heard of Uptrace. Let me tell you about it.Uptrace is an OpenTelemetry-based observability platform that helps developers and Ops users monitor and optimize complex distributed systems. It’s an open-source APM tool that supports distributed tracing, metrics, and logs. Automatic alerts for critical infrastructure can be sent via email, Slack, Telegram, and other notification channels.Uptrace is a great platform for monitoring Redis that you should know about. Let’s dive in.Uptrace is an open source APM available both as an open-source version and an enterprise cloud version; the latter has more features and support options. The open-source version is quite robust as it has Prometheus remote write, AWS CloudWatch, vector logs, percentiles, and alerting, to name just a few examples. To evaluate its use with Redis, we set up open-source deployments of Uptrace and also tested the enterprise edition, called Uptrace Cloud.Its UI looks amazing, and it genuinely is easy to navigate.You can connect to multiple databases but, for our purposes, we drill down to just see the Redis database. It presents us with traces of how our GET, SET, and DEL commands are doing. That provides a quick health check on our cache performance.The Uptrace spans tab helps us visualize command responsiveness down to the millisecond using a heatmap. This gives a quick, easy look at whether a Redis database is performing to expectations. If the heat moves a bit higher (longer millisecond response time), then a bottleneck is starting or inefficiencies are developing.In the image below, this Redis database is completing the majority of GET, SET, and DELETE commands at around 1 millisecond, which is a good target for our cache performance.You can build your own custom metrics that appear on the Uptrace dashboard. For example, memory usage is an important metric for Redis. This screen capture shows how to accomplish that.Uptrace has plenty of other appealing features. One that we found super useful is its optimized storage, which includes performance optimization for common queries, efficient sharding for databases, and improved support for cold storage like S3. Specifically for Redis, Uptrace also supports improved storage policies that give more flexibility to move data between SSD and cold storage; doing so reduces costs, because you don’t have to store as much on SSD.So, what are you waiting for? Try Uptrace as your next OpenTelemetry backend or check out the cloud demo of Uptrace.We offer huge thanks to Vladimir Mihailenco for his many contributions to the DevOps ecosystem, which includes not only Uptrace but also the now-officially-supported Redis Client library for Golang, go-redis. To learn more about the fastest and easiest way to get started with the Go Programming Language and Redis, read Go-Redis Is Now an Official Redis Client or visit the GitHub repo. Happy developing!"
20,https://redis.com/blog/go-redis-official-redis-client/,Go-Redis Is Now an Official Redis Client,"February 15, 2023",Igor Malinovskyi,"The Go-Redis client joins the family of officially supported clients under the Redis umbrella.Do you program in the Go language and want to use Redis? We have good news for you! The open source Go-Redis, which provides a type-safe API for Redis commands, now is easier for you to access and use.Go-Redis is a community-driven project started by Vladimir Mihailenco, whose Uptrace monitoring application creates automatic alerts for complex distributed systems. Community contributors including Dimitrij Denissenko and monkey92t helped make the Go-Redis client a top choice for developers working with Redis.Uptrace is an OpenTelemetry APM with an intuitive query builder, rich dashboards, alerting rules, and integrations for most languages and frameworks. It can process billions of spans and metrics on a single server, allowing you to monitor your applications at 10x lower cost.Beginning with version 9, Go-Redis is hosted under the official Redis organization on GitHub. This change encourages even more collaboration and contributions from the community and ensures that the library stays up-to-date with the latest Redis and Redis Stack features. It also aligns the Go client with other officially supported Redis clients, such as redis-py for Python, nredisstack for .NET, jedis for Java, and node-redis for Node.js.The end result: It’s easier for developers to find and use the appropriate Redis client for their preferred programming language.Existing Go-Redis users should update their imports and dependencies to begin using version 9:If you’re new to Go-Redis, please do explore it! Version 9 adds support for the RESP3 protocol, introduces a new hooks API, improves pipeline retries, and allows performance monitoring via OpenTelemetry.Thank you again to everyone who has been a part of this project thus far. We can’t wait to see what the future holds for the Go client for Redis and the wider Redis community!"
